This is a machine learning library developed bt XIN YAO for CS5350/6350 in University of Utah

1. For DecisionTree: You can choose the dataset and the learning function at HW1.py, and I concrete all functions at functions.py, the caculate method (etropy, Gini index, Mahority Error) can be manually swith by commend or uncommand the sepecific code line.
2. For the LMS: you can set all the parameters like learning rate, and accuracy bond at the Linear.py file, and also choose the different algorithms by select the sepcific functions from linear_function.py file.
3. For Adaboost: I finish the adaboost function at the functions.py file, you can check whether the algorithm is correct based on the theory method. In practice, I didn't finish all code on time, so the HW2.py can call the Adaboost function, but can not run for it.
4. For Perception: You can swith the three different perception methods in HW3.py file by calling different build functions in functions.py and you can manually set different parameters for perception training as well (like learning rate, initial weights, initial bias, Total Epochs). And I also give the results for three methods in Results folder.
5. For SVM, both the primal form and dual form are writen as the function in the functions.py, you can call them by writing the code in the HW4.py, also you can modify the hyperparameters such as value for gamma and C in HW4.py. I separate the dual form with kernel function (kernel.py) and the kenel function combine with the perceptron algorithm (kernel_perceptron.py), these files can run independently. Also, you can change the hyperparameter like gamma value in the python files before running them.
6. For Neual Network, you can change the initialization setting for network's weights in function.py file by changing the commit code line in Neural_Network class, you can get the result for test case, general case and all zero weights case by chaning the initialization. Also, you can change the hyper parameters like gamma, d and number of epochs in backpropagation.py file and implement the backpropagation process with different ways by calling the functions from function.py. You can find the implementation of Neural Network by using both PyTorch and TensorFlow in TensorFlow_PyTorch.py file, the results in report are coming from TensorFlow method, you change the parameters like activation function, activiation method, learning rate and optimization method by calling the embedded function from TensorFlow or PyTorch library.
7. For Logistic Regression, I didn't finish the code for this part, I will try to finish the code for this probelm in the future.
